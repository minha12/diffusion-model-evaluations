# workflow/snakemake/Snakefile

import os
import yaml
from pathlib import Path

# Load configuration
configfile: "config/evaluation.yaml"

# Define dataset
DATASET = config["dataset"]

# Define directories with dataset paths
MODELS_DIR = "models"
RESULTS_DIR = f"results/{DATASET}"
GROUND_TRUTH_DIR = f"data/{DATASET}/patches"
GENERATED_IMAGES_DIR = f"{RESULTS_DIR}/generated_images"
METRICS_DIR = f"{RESULTS_DIR}/metrics"

# Get model names and categorize based on model_class
MODELS = list(config["models"].keys())
SD_MODELS = [model for model in MODELS if config["models"][model].get("model_class", "sd") == "sd"]
LDM_MODELS = [model for model in MODELS if config["models"][model].get("model_class", "sd") == "ldm"]

# Define rule to run all evaluations, including dataset preparation
rule all:
    input:
        f"data/{DATASET}",  # Dataset preparation dependency
        expand(f"{METRICS_DIR}/{{model}}/metrics_results.yaml", model=MODELS),
        f"{RESULTS_DIR}/visualizations"

# Rule to prepare the dataset
rule prepare_dataset:
    output:
        main_dir=directory(f"data/{DATASET}"),
        seg_dir=directory(f"data/{DATASET}/segmentation"),
        patches_dir=directory(f"data/{DATASET}/patches"),
        plain_seg_dir=directory(f"data/{DATASET}/plain-segmentation"),
        prompts=f"data/{DATASET}/prompts/prompts.txt"
    params:
        source_dir=config["data_sources"][DATASET]["path"],
        num_samples=config["dataset_preparation"]["num_samples"],
        seed=config["dataset_preparation"]["seed"]
    conda:
        config["conda_envs"]["default"]
    shell:
        """
        python scripts/preparation/prepare_dataset.py \
            --source_dir={params.source_dir} \
            --target_dir={output.main_dir} \
            --num_samples={params.num_samples} \
            --seed={params.seed}
        """

# Rule to run inference for all models
rule run_all_inference:
    input:
        dataset=f"data/{DATASET}",  # Dataset preparation dependency
        models=expand(f"{GENERATED_IMAGES_DIR}/{{model}}", model=MODELS)
    output:
        touch(f"{RESULTS_DIR}/.inference_complete")

# Rule to clone and prepare the LDM repository (required for LDM models)
rule prepare_ldm:
    output:
        directory("models/latent-diffusion-semantic")
    shell:
        "bash scripts/preparation/prepare_ldm.sh {output}"

# SD inference rule (restricted to SD models)
rule run_sd_inference:
    input:
        accelerate_config=config["accelerate_config"],
        control_images=f"data/{DATASET}/segmentation",
        prompts=f"data/{DATASET}/prompts/prompts.txt",
        dataset=f"data/{DATASET}"
    output:
        directory(f"{GENERATED_IMAGES_DIR}/{{model}}")
    wildcard_constraints:
        model="|".join(SD_MODELS)  # Restrict to SD models
    params:
        batch_size=config["inference"]["batch_size"],
        steps=config["inference"]["num_inference_steps"],
        resolution=config["inference"]["resolution"],
        base_model_path=lambda wildcards: config["models"][wildcards.model]["base_path"],
        model_type=lambda wildcards: config["models"][wildcards.model].get("model_type", "sd"),
        controlnet_path=lambda wildcards: config["models"][wildcards.model].get("controlnet_path", ""),
        seed=config["inference"]["seed"]
    conda:
        config["conda_envs"]["default"]
    resources:
        gpus=4
    shell:
        """
        mkdir -p {output}
        accelerate launch --config_file {input.accelerate_config} scripts/inference/sd_inference.py \
            --condition_dir {input.control_images} \
            --prompts_file {input.prompts} \
            --output_dir {output} \
            --batch_size {params.batch_size} \
            --seed {params.seed} \
            --steps {params.steps} \
            --resolution {params.resolution} \
            --base_model_path {params.base_model_path} \
            --controlnet_path {params.controlnet_path} \
            --model_type {params.model_type}
        """

# LDM inference rule (restricted to LDM models)
rule run_ldm_inference:
    input:
        control_images=f"data/{DATASET}/plain-segmentation",
        dataset=f"data/{DATASET}",
        ldm_repo="models/latent-diffusion-semantic"  # Dependency on LDM repo
    output:
        directory(f"{GENERATED_IMAGES_DIR}/{{model}}")
    wildcard_constraints:
        model="|".join(LDM_MODELS)  # Restrict to LDM models
    params:
        batch_size=config["inference"]["batch_size"],
        config_path=lambda wildcards: config["models"][wildcards.model]["config_path"],
        ckpt_path=lambda wildcards: config["models"][wildcards.model]["ckpt_path"],
        seed=config["inference"]["seed"],
        steps=config["inference"].get("ldm_steps", 200)
    conda:
        config["conda_envs"]["ldm"]
    resources:
        gpus=1
    shell:
        """
        mkdir -p {output}
        python scripts/inference/ldm_inference.py \
            --control_images {input.control_images} \
            --output_dir {output} \
            --batch_size {params.batch_size} \
            --seed {params.seed} \
            --steps {params.steps} \
            --config_path {params.config_path} \
            --ckpt_path {params.ckpt_path}
        """

# Rule to evaluate all models
rule evaluate_all_models:
    input:
        expand(f"{METRICS_DIR}/{{model}}/metrics_results.yaml", model=MODELS)

# Rule to evaluate each model
rule evaluate_model:
    input:
        generated=f"{GENERATED_IMAGES_DIR}/{{model}}",
        ground_truth=GROUND_TRUTH_DIR,
        config="config/metrics.yaml"
    output:
        directory=directory(f"{METRICS_DIR}/{{model}}"),
        results=f"{METRICS_DIR}/{{model}}/metrics_results.yaml"
    conda:
        config["conda_envs"]["default"]
    resources:
        gpus=1
    shell:
        """
        mkdir -p {output.directory}
        python scripts/evaluation/metric_evaluations.py \
            --config {input.config} \
            --generated {input.generated} \
            --ground-truth {input.ground_truth} \
            --output {output.directory}
        """

# Rule to generate comparison report
rule generate_report:
    input:
        metrics=expand(f"{METRICS_DIR}/{{model}}/metrics_results.yaml", model=MODELS)
    output:
        directory(f"{RESULTS_DIR}/visualizations")
    conda:
        config["conda_envs"]["default"]
    shell:
        """
        mkdir -p {output}
        python scripts/evaluation/generate_report.py \
            --metrics-dir {METRICS_DIR} \
            --output-dir {output}
        """        